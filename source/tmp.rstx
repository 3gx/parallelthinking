Massively parallel image compositing
======================================

This is a technical follow-up on my short piece about `interactive
supercomputing <http://tailvega.com/hpc005.html>`_. Here I focus on challenges
we faced, and the solution which allowed us to scale the visualization engine
to thousand of GPUs while maintaining a good degree of interactivity.

Parallel image compositing is one of the final stages of the sort-last parallel
rendering, in which the partial images from separate parallel renderers are blended
into the final image. This is both compute and communication heavy rendering
steps, which is absent when an image is generated by a single renderer.

* `Naive parallel composition`_

  + `Simple implementation`_

* `Optimized parallel composition`_
* `Pipelining`_

Naive parallel composition
-------------------------------

The simplest way to think about the parallel image compositing is to realize
that each of the parallel renderers, which we call ranks, can compose a fixed
set of pixels. With a given image resolution, the number of pixels decreases
linearly with the rank count as :math:`N_{\rm pix} = \frac{W\times\,H}{N_{\rm
rank}}`, where :math:`W` and :math:`H` are image width and height respectively,
and :math:`N_{\rm rank}` is the number of ranks. The amount of data that each
rank receives from remote ones scales inversely with the :math:`N_{\rm rank}`.
Thus, this simple parallel algorithms has a natural strong scalability, at
least in theory. In practice, we are likely to be limited by the interconnect
latency, however.

Simple implementation
^^^^^^^^^^^^^^^^^^^^^^

This section describes a simple naive implementation of the parallel
compositing algorithm, that also shows good scalability in practice. We will
use the following interface

.. code-block:: c++

   void parallelCompositingNaive(
      std::shared_ptr<float4> src,
      std::shared_ptr<float4> dst,
      const int npixels,
      const MPI_Comm &comm,
      const std::vector<int> &order);

The first two arguments to the functions are the pointers to a source and
destination images, and the third argument is the number of pixels to be
composited by each rank. The forth argument is the MPI communicator, through
which ranks will communicate, and the final argument describes the order in
which the images from remote ranks are blended together.

The compilable source of this function can be found `here
<https://github.com/egaburov/parallelthinking/blob/master/source/_code/parallelCompositingNaive.cpp>`_,
and below I will only focus explaining relevant code snippets.


To begin, let's consider a full-image scanline that runs from left to right and
top to bottom:

.. image:: ./_images/scanline_01.jpg
    :width: 600px
    :align: center
    :alt: alternate text test

Let's assume we have eight ranks, and split the scanline into eight pieces of
the same number of pixels each:

.. image:: ./_images/scanline_02.jpg
    :width: 600px
    :align: center

Here, every rank will blend pixels for which it is responsible. For example,
"rank 0", will be blending first 18 pixels, "rank 1" will blend the next set of
18 pixels, and so forth. However, before blending can take place, each rank has
to first send its own pixel to appropriate remote blending rank. The
following image shows a color map of which pixels to be sent to which rank:

.. image:: ./_images/scanline_03.jpg
    :width: 600px
    :align: center

As early, every rank will send first 18 pixel to "rank 0", the next set of 18
pixels to "rank 1", and so forth.

The beauty of this division, is that this communication can be accomplished in
a single MPI collective call:

.. code-block:: c++

   MPI_Alltoall(src.get(), nsend*4, MPI_FLOAT, &colorArray[0], nsend*4, MPI_FLOAT, comm);

where, :code:`colorArray` is declared as :code:`std::vector<float4>` and is of
size :math:`W\times\,H`. The reason is that every rank will receive a fraction
:math:`1/N_{\rm rank}` of all the pixels from all the remote ranks.

Finally, the compositing steps becomes a rather trivial computation, but adding
the pixels from remote rank in  the order specified by :code:`order` argument:

.. code-block:: c++

  for (int i = 0; i < nsend; i++)
  {
    const int stride = i*nrank;
    float4 _dst = {0.0f,0.0f,0.0f,0.0f};
    for (auto p : compositingOrder)
    {
      const float4 &_src = colorArray[p*nsend+i];
      const float f = 1.0f - _dst.w;
      _dst.x += _src.x * f;
      _dst.y += _src.y * f;
      _dst.z += _src.z * f;
      _dst.w += _src.w * f;
    }
    colorArray[i] = _dst;
  }

where :code:`nsend` = :math:`N_{\rm pix}`, and :code:`nrank` = :math:`N_{\rm
rank}`. 

Finally, once  ranks are finished with pixel compositing, these pixels need to
be gathered by a root rank for a display. Due to our linear scanline blending
decomposition, the gathering operation can be done in a single step

.. code-block:: c++

  MPI_Gather(&colorArray[0], nsend*4, MPI_FLOAT, dst.get(), 4*nsend, MPI_FLOAT, 0, comm);

where the final images will populate :code:`dst` buffer. This is very efficient
step, since MPI backend will be responsible for placing data where to the right
place, and not additional data shuffling will be required. Furthermore, with
MPI-3 standard, this can be done in a non-blocking manner, freeing CPU for
other tasks.

The compilable source of this naive compositing function can be found `here
<https://github.com/egaburov/parallelthinking/blob/master/source/_code/parallelCompositingNaive.cpp>`_,


Optimized parallel composition
-------------------------------


Test1

.. figure:: ./_images/logo.png
    :width: 200px
    :align: center
    :height: 100px
    :alt: alternate text test

    **Figure 1**: figure is like an images but with a capture.

Pipelining
------------


Test2

.. math::
  
   W^{3\beta}_{\delta_1 \rho_1 \sigma_2} \approx U^{3\beta}_{\delta_1 \rho_1}

.. code-block:: c++

   class A
   {
     private:
      int foo;
     public:
      A(const int _foo) : foo(_foo) {}
      int get_foo() const {return foo; }
      void set_foo(const int _foo) { foo = _foo; }
   };

   int main(int argc, char * argv[])
   {
     A a(argc);
     retrun 0;
   }

Test code block & math
